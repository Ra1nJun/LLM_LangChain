{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec296ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc24ee9",
   "metadata": {},
   "source": [
    "### CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83022c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import csv\n",
    "\n",
    "# ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# ì¶œë ¥ í˜•ì‹ ì§€ì¹¨ ê°€ì ¸ì˜¤ê¸°\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "prompt.partial_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI ëª¨ë¸ ì„¤ì •\n",
    "model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸, ëª¨ë¸, ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ ìƒì„±\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# \"AI ê´€ë ¨ ê¸°ìˆ \"ì— ëŒ€í•œ ì²´ì¸ í˜¸ì¶œ ì‹¤í–‰\n",
    "result = chain.invoke({\"subject\": \"AI ê´€ë ¨ ê¸°ìˆ \"})\n",
    "\n",
    "# ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n",
    "print(\" AI ê´€ë ¨ ê¸°ìˆ  ëª©ë¡:\")\n",
    "print(result)\n",
    "\n",
    "# ê²°ê³¼ í™œìš© ì˜ˆì‹œ: CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "csv_filename = \"./data/ai_technologies.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"AI ê¸°ìˆ \"])  # í—¤ë” ì¶”ê°€\n",
    "    for item in result:\n",
    "        writer.writerow([item])\n",
    "\n",
    "print(f\" '{csv_filename}' íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7274ee4",
   "metadata": {},
   "source": [
    "### JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "# JSON ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ê³¼í•™ ë¶„ì•¼ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì²´ê³„ì ì´ê³  ê°„ê²°í•œ ë‹µë³€ì„ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.\"),\n",
    "        (\"user\", \"#Format: {format_instructions}\\n\\n#Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# JSON ì¶œë ¥ í˜•ì‹ ì§€ì¹¨ì„ í”„ë¡¬í”„íŠ¸ì— ì ìš©\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75923997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI ëª¨ë¸ ì„¤ì •\n",
    "model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0\n",
    ")\n",
    "# í”„ë¡¬í”„íŠ¸, ëª¨ë¸, ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ëŠ” ì²´ì¸ ìƒì„±\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# ì§ˆë¬¸ ì„¤ì • (ìš°ì£¼ íƒì‚¬ ê´€ë ¨ ì§ˆë¬¸)\n",
    "question = \"ìµœê·¼ 10ë…„ê°„ ì§„í–‰ëœ ì£¼ìš” ìš°ì£¼ íƒì‚¬ ë¯¸ì…˜ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. \\\n",
    "ê° ë¯¸ì…˜ì˜ ì´ë¦„ì€ `mission_name`ì—, ëª©í‘œëŠ” `goal`ì—, ì£¼ê´€ ê¸°ê´€ì€ `agency`ì— ë‹´ì•„ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ ë° JSON ì‘ë‹µ ë°›ê¸°\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# JSON ë°ì´í„° ì¶œë ¥\n",
    "print(json.dumps(response, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec9f7e",
   "metadata": {},
   "source": [
    "### PandasDataFrameOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from typing import Any, Dict\n",
    "import pandas as pd\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "# Pandas DataFrame Output Parser ì„¤ì •\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)\n",
    "\n",
    "# í˜•ì‹ ì§€ì¹¨ ì¶œë ¥\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b30e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" \n",
    "    You are a helpful assistant that interacts with a Pandas DataFrame.\n",
    "    The DataFrame contains the following columns: {columns}.\n",
    "    \n",
    "    Your task is to answer the user's query by generating a command in the following format:\n",
    "    {format_instructions}\n",
    "    \n",
    "    User Query: {query}    \n",
    "    \"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": format_instructions,\n",
    "        \"columns\": \", \".join(df.columns)\n",
    "    },\n",
    ")\n",
    "print(prompt.partial_variables['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´ì¸ ìƒì„±\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# ëª¨ë¸ ì‘ë‹µ ë°›ê¸°\n",
    "try:\n",
    "    # **Name ì—´ì„ í‘œì‹œí•˜ì‹­ì‹œì˜¤.**\n",
    "    print('Name ì»¬ëŸ¼ ì¶œë ¥')\n",
    "    df_query = \"Show the Name column\"\n",
    "\n",
    "    parser_output = chain.invoke({\"query\": df_query})\n",
    "    print(type(parser_output))\n",
    "    print(parser_output)\n",
    "\n",
    "    # **ì²«ë²ˆì§¸ í–‰ì„ í‘œì‹œí•˜ì‹­ì‹œì˜¤.**\n",
    "    print('ì²«ë²ˆì§¸ í–‰ ì¶œë ¥')\n",
    "    df_query2 = \"Show first row\"\n",
    "\n",
    "    parser_output2 = chain.invoke({\"query\": df_query2})\n",
    "    print(parser_output2)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜ {data : [{},{},{}] }\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"data\", description=\"A list of dictionaries representing table rows.\"),\n",
    "]\n",
    "\n",
    "# Output Parser ì„¤ì •\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an AI assistant that generates tabular data. \n",
    "    You must return the data in JSON format that follows this schema:\n",
    "    \n",
    "    {format_instructions}\n",
    "        \n",
    "    **User Query:**\n",
    "    {query}\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6303bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´ì¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ â†’ ëª¨ë¸ â†’ OutputParser)\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# ì‹¤í–‰ í•¨ìˆ˜\n",
    "def generate_dataframe(user_query):\n",
    "    try:\n",
    "        # ëª¨ë¸ í˜¸ì¶œ\n",
    "        json_response = chain.invoke({\"query\": user_query})\n",
    "        print(json_response)\n",
    "        \n",
    "        # ëª¨ë¸ì´ ë°˜í™˜í•œ JSONì„ Pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        df = pd.DataFrame(json_response[\"data\"])\n",
    "\n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(\"\\nğŸ”¹ Generated DataFrame:\\n\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1375a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ì˜ˆì œ 1] 2024ë…„ ìƒë°˜ê¸° ì„œìš¸ ì•„íŒŒíŠ¸ í‰ê·  ë§¤ë§¤ ê°€ê²© ë°ì´í„° ìƒì„±\n",
    "print('2024ë…„ í•˜ë°˜ê¸° ì„œìš¸ ì•„íŒŒíŠ¸ í‰ê·  ë§¤ë§¤ ê°€ê²© ë°ì´í„° ìƒì„±')\n",
    "df_seoul_housing = generate_dataframe(\n",
    "    \"Create a dataset of the average apartment sale prices in Seoul for the second half of 2024 with columns: District (êµ¬), Average Price (in KRW), Number of Transactions, and Year-over-Year Change (%).\"\n",
    ")\n",
    "print(df_seoul_housing.shape)\n",
    "df_seoul_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8349a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2024ë…„ ì„œìš¸ ì§€í•˜ì² ì—­ë³„ ìœ ë™ ì¸êµ¬ ë°ì´í„°')\n",
    "# [ì˜ˆì œ 2] 2024ë…„ ì„œìš¸ ì§€í•˜ì² ì—­ë³„ ìœ ë™ ì¸êµ¬ ë°ì´í„°\n",
    "df_seoul_subway = generate_dataframe(\n",
    "    \"Generate a dataset of the top 10 busiest subway stations in Seoul in 2024 with columns: Station Name, Line Number, Daily Passenger Volume, and Weekday vs Weekend Ratio.\"\n",
    ")\n",
    "if df_seoul_subway is not None:\n",
    "    # print(df_seoul_subway.shape)\n",
    "    df_seoul_subway.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('í•œêµ­ 5ëŒ€ í¸ì˜ì  ë¸Œëœë“œë³„ 2024ë…„ ë§¤ì¶œ ë° ì í¬ ìˆ˜')\n",
    "# [ì˜ˆì œ 3] í•œêµ­ 5ëŒ€ í¸ì˜ì  ë¸Œëœë“œë³„ 2024ë…„ ë§¤ì¶œ ë° ì í¬ ìˆ˜\n",
    "df_korean_convenience_stores = generate_dataframe(\n",
    "    \"Create a dataset of the top 5 convenience store brands in Korea in 2024 with columns: Brand Name, Number of Stores, Total Revenue (in billion KRW), and Market Share (%).\"\n",
    ")\n",
    "df_korean_convenience_stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30758d",
   "metadata": {},
   "source": [
    "### PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94102728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸\n",
    "class MovieRecommendation(BaseModel):\n",
    "    movie_title: str = Field(description=\"ì¶”ì²œ ì˜í™” ì œëª©\")\n",
    "    reason: str = Field(description=\"ì¶”ì²œ ì´ìœ \")\n",
    "    genre: List[str] = Field(description=\"ì˜í™” ì¥ë¥´\")\n",
    "    estimated_rating: float = Field(description=\"10ì  ë§Œì ì—ì„œ ì˜ˆìƒ í‰ì \")\n",
    "    \n",
    "# Pydantic ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”\n",
    "parser = PydanticOutputParser(pydantic_object=MovieRecommendation)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "template = \"\"\"\n",
    "ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì˜í™”ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n",
    "ìš”ì²­: {query}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# íŒŒì„œì˜ ì§€ì‹œì‚¬í•­ì„ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…\n",
    "prompt = prompt.partial(\n",
    "    format_instructions=parser.get_format_instructions()\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰\n",
    "query = \"1990ë…„ëŒ€ í´ë˜ì‹í•œ ëŠë‚Œì˜ ê³µí¬ ì˜í™” ì¶”ì²œí•´ì¤˜\"\n",
    "chain = prompt | model | parser\n",
    "output = chain.invoke({\"query\": query})\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ì¶”ì²œ ì˜í™”: {output.movie_title}\")\n",
    "print(f\"ì¶”ì²œ ì´ìœ : {output.reason}\")\n",
    "print(f\"ì¥ë¥´: {', '.join(output.genre)}\")\n",
    "print(f\"ì˜ˆìƒ í‰ì : {output.estimated_rating}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dce39a",
   "metadata": {},
   "source": [
    "### StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4772c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# ì¶œë ¥ êµ¬ì¡° ì •ì˜ (í‰ì , ì¥ì , ë‹¨ì , ìš”ì•½)\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"rating\", description=\"5ì  ë§Œì ì—ì„œ ì˜ˆìƒ í‰ì \"),\n",
    "    ResponseSchema(name=\"pros\", description=\"ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ ì¥ì  3ê°€ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥\"),\n",
    "    ResponseSchema(name=\"cons\", description=\"ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ ë‹¨ì  3ê°€ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥\"),\n",
    "    ResponseSchema(name=\"summary\", description=\"ë¦¬ë·°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\")\n",
    "]\n",
    "\n",
    "# íŒŒì„œ ì´ˆê¸°í™”\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "print(\"ì¶œë ¥ í˜•ì‹ ì§€ì‹œì‚¬í•­:\")\n",
    "print(format_instructions)\n",
    "template = \"\"\"\n",
    "ë‹¤ìŒ ì œí’ˆ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì„¸ìš”. ë¦¬ë·° ë‚´ìš©: {review}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™” (temperature=0.5ë¡œ ì„¤ì •í•´ ì¼ê´€ì„± ìˆëŠ” ì¶œë ¥)\n",
    "#model = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¦¬ë·° ë°ì´í„°\n",
    "review = \"\"\"\n",
    "ì´ ìŠ¤ë§ˆíŠ¸í°ì€ ë°°í„°ë¦¬ ìˆ˜ëª…ì´ ì •ë§ ì¢‹ì•„ì„œ í•˜ë£¨ ì¢…ì¼ ì‚¬ìš©í•´ë„ ì¶©ì „ì´ í•„ìš” ì—†ì—ˆì–´ìš”. \n",
    "ì¹´ë©”ë¼ í™”ì§ˆë„ ì„ ëª…í•˜ê³ , íŠ¹íˆ ì•¼ê°„ ëª¨ë“œê°€ í›Œë¥­í•©ë‹ˆë‹¤. \n",
    "ë‹¤ë§Œ ê°€ê²©ì´ ì¡°ê¸ˆ ë¹„ì‹¸ê³ , ë¬´ê²Œê°€ 200gì´ ë„˜ì–´ì„œ ì†ì´ í”¼ê³¤í•  ìˆ˜ ìˆì–´ìš”.\n",
    "\"\"\"\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "chain = prompt | model | parser\n",
    "\n",
    "output = chain.invoke({\"review\": review})\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥ (Pretty Print)\n",
    "print(\"===== ë¶„ì„ ê²°ê³¼ =====\")\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b071b",
   "metadata": {},
   "source": [
    "### DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0abd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(datetime.today())\n",
    "\n",
    "# ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™” (ì‹œê°„ëŒ€ í¬í•¨ ê°€ëŠ¥)\n",
    "datetime_parser = DatetimeOutputParser()\n",
    "format_instructions = datetime_parser.get_format_instructions()\n",
    "\n",
    "print(\"ë‚ ì§œ ì¶œë ¥ í˜•ì‹ ì§€ì‹œì‚¬í•­:\")\n",
    "print(format_instructions)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "# í˜„ì¬ ë‚ ì§œë¥¼ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "template = f\"\"\"\n",
    "í˜„ì¬ ë‚ ì§œ: {current_date}\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ìƒëŒ€ì  í‘œí˜„(ì˜ˆ: 'ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼')ì€ í˜„ì¬ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸: {{text}}\n",
    "\n",
    "{{format_instructions}}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™” (temperature=0.1ë¡œ ì„¤ì •í•´ ì •í™•í•œ ë‚ ì§œ ì¶œë ¥ ê°•ì¡°)\n",
    "#model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° (ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ í¬í•¨)\n",
    "texts = [\n",
    "    \"íšŒì˜ëŠ” 2025ë…„ 6ì›” 15ì¼ ì˜¤í›„ 2ì‹œì— ì˜ˆì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"í”„ë¡œì íŠ¸ ë§ˆê°ì¼ì€ ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ì…ë‹ˆë‹¤.\",\n",
    "    \"í–‰ì‚¬ ì‹œì‘: 7/25/2025 18:00 KST\",\n",
    "    \"3ì¼ í›„ì— ì‹œìŠ¤í…œ ì ê²€ì´ ì§„í–‰ë©ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "chain = prompt | model | datetime_parser\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"\\nì›ë³¸ í…ìŠ¤íŠ¸: {text}\")\n",
    "    output = chain.invoke({\"text\": text})\n",
    "    print(f\"ì¶”ì¶œëœ ë‚ ì§œ: {output.strftime('%Y-%m-%d %H:%M:%S %Z')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë²¤íŠ¸ ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "event_template = \"\"\"\n",
    "í˜„ì¬ ë‚ ì§œ: {current_date}\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ì´ë²¤íŠ¸ì˜ ë‚ ì§œ/ì‹œê°„ì„ ì¶”ì¶œí•˜ì„¸ìš”. ê° ì´ë²¤íŠ¸ëŠ” ì´ë¦„ê³¼ ë‚ ì§œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ê° ì´ë²¤íŠ¸ëŠ” í˜„ì¬ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸: {text}\n",
    "\n",
    "ì¶œë ¥ í˜•ì‹:\n",
    "- ì´ë²¤íŠ¸ëª…: [ì´ë¦„]\n",
    "- ë‚ ì§œ: [YYYY-MM-DD HH:MM:SS]\n",
    "\"\"\"\n",
    "\n",
    "event_prompt = ChatPromptTemplate.from_template(event_template)\n",
    "event_chain = event_prompt | model\n",
    "\n",
    "# ì˜ˆì‹œ í…ìŠ¤íŠ¸ (ì—¬ëŸ¬ ì´ë²¤íŠ¸ í¬í•¨)\n",
    "event_text = \"\"\"\n",
    "12ì›” 10ì¼ì— í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ë§ˆì¼“ì´ ì—´ë¦¬ê³ , 12ì›” 24ì¼ì—ëŠ” í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì´ë¸Œ íŒŒí‹°ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "ë˜í•œ 1ì›” 1ì¼ 00:00ì— ìƒˆí•´ ì¹´ìš´íŠ¸ë‹¤ìš´ì´ ì§„í–‰ë  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "print(event_chain.invoke({\"current_date\":current_date, \"text\": event_text}).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e269b",
   "metadata": {},
   "source": [
    "### EnumOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import EnumOutputParser, OutputFixingParser\n",
    "from langchain.schema import OutputParserException\n",
    "\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "# ê°ì • í´ë˜ìŠ¤ ì •ì˜ (Enum)\n",
    "class Sentiment(str, Enum):\n",
    "    POSITIVE = \"ê¸ì •\"\n",
    "    NEGATIVE = \"ë¶€ì •\"\n",
    "    NEUTRAL = \"ì¤‘ë¦½\"\n",
    "\n",
    "# EnumOutputParser ì´ˆê¸°í™”\n",
    "enumParser = EnumOutputParser(enum=Sentiment)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "print(\"ê°ì • ë¶„ë¥˜ ì¶œë ¥ í˜•ì‹:\")\n",
    "print(format_instructions)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "template = \"\"\"\n",
    "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ê³ , ë°˜ë“œì‹œ ì•„ë˜ ì„¸ ê°€ì§€ ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "í…ìŠ¤íŠ¸: \"{text}\"\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "ì¤‘ìš” ê·œì¹™:\n",
    "1. ë°˜ë“œì‹œ \"ê¸ì •\", \"ë¶€ì •\", \"ì¤‘ë¦½\" ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”\n",
    "2. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì„¤ëª…ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”\n",
    "3. ì´ëª¨ì§€ë‚˜ íŠ¹ìˆ˜ë¬¸ìë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n",
    "4. ì˜¤ì§ í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(format_instructions=format_instructions)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0  # ì¼ê´€ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •\n",
    ")\n",
    "\n",
    "# OutputFixingParserë¡œ ì•ˆì •ì„± í–¥ìƒ\n",
    "fixing_parser = OutputFixingParser.from_llm(parser=enumParser, llm=model)\n",
    "\n",
    "print(\"ëª¨ë¸ ë° íŒŒì„œ ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸\n",
    "texts = [\n",
    "    \"ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”! ì™„ì „ ë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”.\",\n",
    "    \"ì„œë¹„ìŠ¤ê°€ ë„ˆë¬´ ëŠë¦¬ê³  ë¶ˆì¹œì ˆí–ˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì˜¤ëŠ˜ì€ ë¹„ê°€ ì˜¨ë‹¤ë„¤ìš”.\",\n",
    "    \"ë°°ì†¡ì€ ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆì´ ì•„ì‰½ìŠµë‹ˆë‹¤.\",\n",
    "    \"ìµœê³ ì˜ ê²½í—˜ì´ì—ˆìŠµë‹ˆë‹¤!\",\n",
    "    \"ì™„ì „ ì‹¤ë§í–ˆì–´ìš”... ìµœì•…ì´ì—ìš”\"\n",
    "]\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ {len(texts)}ê°œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•ˆì „í•œ ê°ì • ë¶„ì„ í•¨ìˆ˜ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)\n",
    "def safe_sentiment_analysis(text, use_fixing_parser=True):\n",
    "    \"\"\"ì•ˆì „í•œ ê°ì • ë¶„ì„ í•¨ìˆ˜ - ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨\"\"\"\n",
    "    try:\n",
    "        # ê¸°ë³¸ ì²´ì¸ ìƒì„±\n",
    "        chain = prompt | model | (fixing_parser if use_fixing_parser else enumParser)\n",
    "        \n",
    "        # ë¶„ì„ ì‹¤í–‰\n",
    "        result = chain.invoke({\"text\": text})\n",
    "        return result, None\n",
    "        \n",
    "    except OutputParserException as e:\n",
    "        return None, f\"íŒŒì‹± ì˜¤ë¥˜: {str(e)[:100]}...\"\n",
    "    except Exception as e:\n",
    "        return None, f\"ì¼ë°˜ ì˜¤ë¥˜: {str(e)[:100]}...\"\n",
    "\n",
    "# ì‹¤ì œ ê°ì • ë¶„ì„ ì‹¤í–‰ (API í‚¤ í•„ìš”)\n",
    "def run_sentiment_analysis():\n",
    "    \"\"\"ì‹¤ì œ ê°ì • ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "    print(\"=== ì‹¤ì œ ê°ì • ë¶„ì„ ê²°ê³¼ ===\")\n",
    "    \n",
    "    success_count = 0\n",
    "    total_count = len(texts)\n",
    "    \n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"\\n{i}. í…ìŠ¤íŠ¸: {text}\")\n",
    "        \n",
    "        # OutputFixingParser ì‚¬ìš©\n",
    "        result, error = safe_sentiment_analysis(text, use_fixing_parser=True)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"   ê°ì •: {result.value} \")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"   ì˜¤ë¥˜: {error} \")\n",
    "            \n",
    "            # ê¸°ë³¸ íŒŒì„œë¡œ ì¬ì‹œë„\n",
    "            print(\"   ê¸°ë³¸ íŒŒì„œë¡œ ì¬ì‹œë„...\")\n",
    "            result2, error2 = safe_sentiment_analysis(text, use_fixing_parser=False)\n",
    "            \n",
    "            if result2:\n",
    "                print(f\"   ê°ì •: {result2.value} (ê¸°ë³¸ íŒŒì„œ ì„±ê³µ)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ì¬ì‹œë„ ì‹¤íŒ¨: {error2} \")\n",
    "    \n",
    "    print(f\"\\n=== ê²°ê³¼ ìš”ì•½ ===\")\n",
    "    print(f\"ì„±ê³µ: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)\")\n",
    "    print(f\"ì‹¤íŒ¨: {total_count-success_count}/{total_count}\")\n",
    "\n",
    "# ì‹¤ì œ ë¶„ì„ ì‹¤í–‰ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)\n",
    "try:\n",
    "    run_sentiment_analysis()\n",
    "except Exception as e:\n",
    "    print(\"API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜:\")\n",
    "    print(\"ì‹¤ì œ ì‹¤í–‰ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\")\n",
    "    print(f\"ì˜¤ë¥˜ ìƒì„¸: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a46a2",
   "metadata": {},
   "source": [
    "### BooleanOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import BooleanOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Boolean íŒŒì„œ ì´ˆê¸°í™”\n",
    "boolParser = BooleanOutputParser()\n",
    "\n",
    "# ìˆ˜ë™ìœ¼ë¡œ í¬ë§· ì§€ì‹œì‚¬í•­ ì •ì˜ (LangChain ë²„ì „ ì´ìŠˆ íšŒí”¼)\n",
    "format_instructions = \"\"\"\n",
    "ì¶œë ¥ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤:\n",
    "- `True`: ëª¨ë“  ì¡°ê±´ ì¶©ì¡± ì‹œ\n",
    "- `False`: í•˜ë‚˜ë¼ë„ ì¡°ê±´ ë¶ˆì¶©ì¡± ì‹œ\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "True  # ëª¨ë“  ì¡°ê±´ ë§Œì¡±\n",
    "False # ì¡°ê±´ ë¶ˆë§Œì¡±\n",
    "\"\"\"\n",
    "\n",
    "# ìŠ¹ì¸/ê±°ë¶€ ê²°ì • í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "template = \"\"\"\n",
    "ë‹¤ìŒ ëŒ€ì¶œ ì‹ ì²­ìë¥¼ í‰ê°€í•˜ì„¸ìš”. ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ë©´ `True`, ì•„ë‹ˆë©´ `False`ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "### ì¡°ê±´:\n",
    "1. ë‚˜ì´ >= {min_age}ì„¸\n",
    "2. ì‹ ìš© ì ìˆ˜ >= {min_credit_score}\n",
    "3. ì›” ìˆ˜ì… >= ${min_income}\n",
    "\n",
    "### ì‹ ì²­ì ì •ë³´:\n",
    "{applicant_details}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(format_instructions=format_instructions)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "chain = prompt | model | boolParser\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "test_cases = [\n",
    "    {\n",
    "        \"min_age\": 18,\n",
    "        \"min_credit_score\": 700,\n",
    "        \"min_income\": 3000,\n",
    "        \"applicant_details\": \"\"\"\n",
    "        - ì´ë¦„: ê¹€ì² ìˆ˜\n",
    "        - ë‚˜ì´: 25ì„¸\n",
    "        - ì‹ ìš© ì ìˆ˜: 750\n",
    "        - ì›” ìˆ˜ì…: $3,500\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"min_age\": 18,\n",
    "        \"min_credit_score\": 700,\n",
    "        \"min_income\": 3000,\n",
    "        \"applicant_details\": \"\"\"\n",
    "        - ì´ë¦„: ì´ì˜í¬\n",
    "        - ë‚˜ì´: 17ì„¸\n",
    "        - ì‹ ìš© ì ìˆ˜: 680\n",
    "        - ì›” ìˆ˜ì…: $2,800\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ê±°ë¶€ ì‚¬ìœ  ìƒì„± í”„ë¡¬í”„íŠ¸ (ì¶œë ¥ í˜•ì‹ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •)\n",
    "reason_template = \"\"\"\n",
    "ë‹¤ìŒ ëŒ€ì¶œ ì‹ ì²­ ê±°ë¶€ ì‚¬ìœ ë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”. ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "[ê±°ë¶€ ì‚¬ìœ ]: [ì‚¬ìœ  ë‚´ìš©]\n",
    "\n",
    "### ì‹ ì²­ì ì •ë³´:\n",
    "{applicant_details}\n",
    "\n",
    "### ì¡°ê±´:\n",
    "- ìµœì†Œ ë‚˜ì´: {min_age}ì„¸\n",
    "- ìµœì†Œ ì‹ ìš© ì ìˆ˜: {min_credit_score}\n",
    "- ìµœì†Œ ì›” ìˆ˜ì…: ${min_income}\n",
    "\"\"\"\n",
    "reason_prompt = ChatPromptTemplate.from_template(reason_template)\n",
    "reason_chain = reason_prompt | model | StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ í™•ì¥ (Boolean íŒŒì„œì™€ ë¶„ë¦¬)\n",
    "def get_decision_with_reason(input_dict):\n",
    "    # 1. ë¨¼ì € Boolean ê²°ì •\n",
    "    decision = chain.invoke(input_dict)\n",
    "    \n",
    "    # 2. ê±°ë¶€ ì‹œì—ë§Œ ì‚¬ìœ  ìƒì„±\n",
    "    if not decision:\n",
    "        try:\n",
    "            reason = reason_chain.invoke({\n",
    "                \"applicant_details\": input_dict[\"applicant_details\"],\n",
    "                \"min_age\": input_dict[\"min_age\"],\n",
    "                \"min_credit_score\": input_dict[\"min_credit_score\"],\n",
    "                \"min_income\": input_dict[\"min_income\"]\n",
    "            })\n",
    "            return decision, reason\n",
    "        except Exception as e:\n",
    "            return decision, f\"ê±°ë¶€ ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨: {str(e)}\"\n",
    "    return decision, \"ëª¨ë“  ì¡°ê±´ì„ ì¶©ì¡±í–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ (ì•ˆì „í•œ ì‹¤í–‰)\n",
    "try:\n",
    "    decision, reason = get_decision_with_reason(test_cases[1])\n",
    "    print(f\"\\nê²°ê³¼: {'ìŠ¹ì¸' if decision else 'ê±°ë¶€'}\")\n",
    "    print(f\"ì‚¬ìœ : {reason}\")\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
